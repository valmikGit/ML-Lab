{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading of the CSV file into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv('train.csv')\n",
    "train_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING:\n",
    "1) Removing duplicate rows.\n",
    "2) The columns where the null value percentage is less than 5% we should remove the rows where these null values are present.\n",
    "3) The columns where the null value percentage is more than equal to 5% and less than 30%, we should use imputation techniques to remove null values.\n",
    "4) The columns where the null value percentage is more than equal to 30%, we should drop the entire columns because a large amount of the columns are null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_value_percentages=(train_csv.isna().sum()/train_csv.shape[0])*100\n",
    "null_value_percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_drop=null_value_percentages[null_value_percentages<5].sort_values(ascending=False)\n",
    "rows_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_drop = rows_to_drop.keys()\n",
    "rows_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in rows_to_drop:\n",
    "\tif(null_value_percentages[row]<5):\n",
    "\t\ttrain_csv.drop(labels=train_csv.index[train_csv[row].isna()],inplace=True)\n",
    "train_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBSERVATION: There are 9 rows for which null value percentage is less than 5% and  more than 0%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop=null_value_percentages[null_value_percentages>30]\n",
    "columns_to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBSERVATION: There are no columns for which null value percentage is more than 30%.\n",
    "As there were no columns dropped, there are no chances of having duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_value_percentages=(train_csv.isna().sum()/train_csv.shape[0])*100\n",
    "null_value_percentages=null_value_percentages[null_value_percentages>0]\n",
    "columns_to_impute=null_value_percentages.keys()\n",
    "columns_to_impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in columns_to_impute:\n",
    "\tif(train_csv[column].dtype== object):\n",
    "\t\tprint(\"Column: \",column,\"\\tCounts:\\n\", train_csv[column].value_counts(dropna=False))\n",
    "\t\tprint(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBSERVATION: There are no categorical variables according to the code above. However, Feature2 is a categorical variable which has boolean values i.e. TRUE and FALSE. I will focus on Feature2 particularly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv['Feature2'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBSERVATION: Thus, there are 0 null values in the column 'Feature2'.\n",
    "It has TRUE or FALSE values. Thus, to convert the categorical variable into a numerical variable, I will assign TRUE = 1 and FALSE = 0.\n",
    "All the categorical variables must be converted to numerical variables before finding the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using apply with a lambda function\n",
    "train_csv['Feature2'] = train_csv['Feature2'].apply(lambda x: 1 if x == True else 0)\n",
    "train_csv['Feature2']\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# encoder = OneHotEncoder()\n",
    "# categorical_features = encoder.fit_transform(train_csv['Feature2'])\n",
    "# train_csv['Feature2'] = categorical_features\n",
    "# train_csv['Feature2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all the categorical variables have been converted into numerical variables.\n",
    "Now I have to handle columns where the null value percentage is more than 5% and less than 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_value_percentages=(train_csv.isna().sum()/train_csv.shape[0])*100\n",
    "null_value_percentages=null_value_percentages[null_value_percentages>0]\n",
    "null_value_percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean imputation: Best for numerical data that is symmetrically distributed (i.e., normally distributed) without outliers. \n",
    "Median imputation: Best for numerical data that is skewed (non-symmetric) or contains outliers. \n",
    "Mode imputation: Best for categorical data or numerical data with discrete, frequently occurring values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(train_csv['Feature4'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are no outliers in Feature4 and it is a numerical variable. Thus, I will impute the values using mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_value = train_csv['Feature4'].mean()\n",
    "train_csv['Feature4'] = train_csv['Feature4'].fillna(mean_value)\n",
    "train_csv['Feature4'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will keep all the categorical variables as the initial columns and the numerical variables as the columns in the last.\n",
    "This will help me to standardize and scale only the numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to swap\n",
    "col1, col2 = 'Feature1', 'Feature2'\n",
    "\n",
    "# Swap the columns\n",
    "columns = list(train_csv.columns)  # Get the list of column names\n",
    "col1_index, col2_index = columns.index(col1), columns.index(col2)  # Find the indices of the columns\n",
    "\n",
    "# Swap the columns in the list\n",
    "columns[col1_index], columns[col2_index] = columns[col2_index], columns[col1_index]\n",
    "\n",
    "# Reorder DataFrame columns based on the modified list\n",
    "train_csv = train_csv[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will split the given data into the features and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_csv.iloc[:,:-1]\n",
    "Y = train_csv.iloc[:,-1]\n",
    "print(X.head())\n",
    "print(Y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into 70% for training and 30% for testing with random seed as 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1)\n",
    "print(X_train.head())\n",
    "print(X_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The X_train should be normalized (fitted and transformed). Then, using the same scaler we have to transform (only transform, not fit) the X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# X_train[:, 1:] = scaler.fit_transform(X_train[:, 1:])\n",
    "# X_test[:, 1:] = scaler.transform(X_test[:, 1:])\n",
    "# X_test\n",
    "# AVOID STANDARD SCALING ON CATEGORICAL VARIABLES.\n",
    "\n",
    "# Use .iloc for indexing when working with DataFrames\n",
    "# This selects all rows and columns starting from the second column (index 1)\n",
    "X_train_scaled = scaler.fit_transform(X_train.iloc[:, 1:])\n",
    "X_test_scaled = scaler.transform(X_test.iloc[:, 1:])\n",
    "\n",
    "# If you need to reassign back to the original DataFrame or join with unscaled data:\n",
    "X_train.iloc[:, 1:] = X_train_scaled\n",
    "X_test.iloc[:, 1:] = X_test_scaled\n",
    "\n",
    "print(X_train.head())\n",
    "print(X_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten the data for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data points for plotting\n",
    "\n",
    "#np.argsort() takes 1D arrays\n",
    "#returns indices that sort X_train in ascending order\n",
    "# sorted_indices = np.argsort(X_train.flatten())\n",
    "\n",
    "#X_train is sorted as per the indices\n",
    "#Y_train is also sorted as per the same indices, so that X-Y values align\n",
    "# X_train_sorted = X_train[sorted_indices]\n",
    "# Y_train_sorted = Y_train[sorted_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regression = LinearRegression()\n",
    "regression.fit(X_train, Y_train)\n",
    "print(regression.coef_)\n",
    "\n",
    "print(regression.intercept_)\n",
    "\n",
    "print(regression.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pred = regression.predict(X_test)\n",
    "\n",
    "plt.scatter(x=Y_test, y=reg_pred)\n",
    "plt.xlabel('Y test data')\n",
    "plt.ylabel('Predicted values by the linear regression ')\n",
    "# Plotting the prediction vs Y test data. It should ideally resemble y = x line.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = Y_test - reg_pred\n",
    "residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will find the mean squared error, mean absolute error and the square root of mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(mean_squared_error(Y_test, reg_pred))\n",
    "print(mean_absolute_error(Y_test, reg_pred))\n",
    "print(np.sqrt(mean_squared_error(Y_test, reg_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R square and adjusted R square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "score = r2_score(y_true=Y_test, y_pred=reg_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_r2 = 1 - (1 - score)*((len(Y_test) - 1)/(len(Y_test) - X_test.shape[1] - 1))\n",
    "print(adjusted_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# X_train[:, 1:] = scaler.fit_transform(X_train[:, 1:])\n",
    "# X_test[:, 1:] = scaler.transform(X_test[:, 1:])\n",
    "# X_test\n",
    "# AVOID STANDARD SCALING ON CATEGORICAL VARIABLES.\n",
    "\n",
    "# Use .iloc for indexing when working with DataFrames\n",
    "# This selects all rows and columns starting from the second column (index 1)\n",
    "X_train_scaled = scaler.fit_transform(X_train.iloc[:, 1:])\n",
    "X_test_scaled = scaler.transform(X_test.iloc[:, 1:])\n",
    "\n",
    "# If you need to reassign back to the original DataFrame or join with unscaled data:\n",
    "X_train.iloc[:, 1:] = X_train_scaled\n",
    "X_test.iloc[:, 1:] = X_test_scaled\n",
    "\n",
    "print(X_train.head())\n",
    "print(X_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READING OF TESTING CSV FILE AND PREPROCESSING OF THE DATA READ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv = pd.read_csv('test.csv')\n",
    "test_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to swap\n",
    "col1, col2 = 'Feature1', 'Feature2'\n",
    "\n",
    "# Swap the columns\n",
    "columns = list(test_csv.columns)  # Get the list of column names\n",
    "col1_index, col2_index = columns.index(col1), columns.index(col2)  # Find the indices of the columns\n",
    "\n",
    "# Swap the columns in the list\n",
    "columns[col1_index], columns[col2_index] = columns[col2_index], columns[col1_index]\n",
    "\n",
    "# Reorder DataFrame columns based on the modified list\n",
    "test_csv = test_csv[columns]\n",
    "test_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv['Feature2'] = test_csv['Feature2'].apply(lambda x: 1 if x == True else 0)\n",
    "\n",
    "test_csv.drop(columns=['id'], inplace=True)\n",
    "test_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "test_csv.iloc[:, 1:] = test_csv.iloc[:, 1:].astype(float)\n",
    "\n",
    "test_scaled = scaler.fit_transform(test_csv.iloc[:, 1:])\n",
    "test_csv.iloc[:, 1:] = test_scaled\n",
    "test_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POLYNOMIAL REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "# import numpy as np\n",
    "\n",
    "# Assuming X_train, X_test, Y_train, Y_test are already defined and scaled if necessary\n",
    "\n",
    "# Polynomial Regression - Different Orders\n",
    "orders = []\n",
    "for i in range(1, 11):\n",
    "    orders.append(i)\n",
    "\n",
    "# Lists to store the errors and R2 scores\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "r2_score_list = []\n",
    "# final_test = []\n",
    "\n",
    "for order in orders:\n",
    "    # Create an instance of PolynomialFeatures with the specified degree\n",
    "    poly_features = PolynomialFeatures(degree=order)\n",
    "    X_train_poly = poly_features.fit_transform(X=X_train)\n",
    "    X_test_poly = poly_features.transform(X=X_test)\n",
    "\n",
    "    # test_poly = poly_features.fit_transform(X=X_test)\n",
    "    # final_test = test_poly\n",
    "\n",
    "    ridge_reg = Ridge(alpha=0.4)\n",
    "    ridge_reg.fit(X=X_train_poly, y=Y_train)\n",
    "\n",
    "    # y_pred = ridge_reg.predict(poly_features.transform(X=X_test_poly))\n",
    "    # y_pred = ridge_reg.predict(poly_features.fit_transform(X=test_csv))\n",
    "\n",
    "    print(f'Order = {order}, Training Ridge score = {ridge_reg.score(X=X_train_poly, y=Y_train)}, Testing Ridge score = {ridge_reg.score(X=X_test_poly, y=Y_test)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
